{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 - 14.3 \n",
    "Compute using $h=2^{-1}, 2^{-2}, \\dots 2^{-5}$ and the forward, backward, and centered difference approximations the following derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f(x) = \\sqrt{x}$ at $x=0.5$.  The answer is $f'(0.5) = 2^{-1/2} \\approx 0.70710678118$.\n",
    "- $f(x) = \\arctan(x^2 - 0.9  x + 2)$ at $x=0.5$.  The answer is $f'(0.5) = \\frac{5}{212}$.\n",
    "- $f(x) = J_0(x),$ at $x=1$, where $J_0(x)$ is a Bessel function of the first kind given by $$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\, \\Gamma(m+\\alpha+1)} {\\left(\\frac{x}{2}\\right)}^{2m+\\alpha}.  $$ The answer is $f'(1) \\approx -0.4400505857449335.$\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is split up into the three given derivativesThis procedure is modeled to the one in found starting at page 239 of the lecture notes.\n",
    "\n",
    "We will make a function that can take the necessary inputs for each part and calculate the difference approximation for each h. It will also plot the absolute errors, which are not required for full credit.\n",
    "\n",
    "Here I also include $2^{-6}$ to $2^{-10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cmath\n",
    "%matplotlib inline\n",
    "\n",
    "def DiffApproximations(f,h,x,exact,loud=True,plot=True):\n",
    "    \n",
    "    \"\"\"Determines the forward, backward, and centered difference\n",
    "    approximations for a given set of steps. Also prints the results\n",
    "    and plots the absolute errors if requested\n",
    "    \n",
    "    Args:\n",
    "        f: function to approximate the derivative of\n",
    "        h: numpy array of step sizes\n",
    "        x: point at which to approximate\n",
    "        exact: the exact value at x for comparison purposes\n",
    "        loud: bool of whether to print a table of the results\n",
    "        plot: bool of whether to plot the errors\n",
    "        \n",
    "    Returns:\n",
    "        forward: numpy array of the forward approximations\n",
    "        backward: numpy array of the backward approximations\n",
    "        centered: numpy array of the centered approximations\"\"\"\n",
    "\n",
    "    # Define variables to store approximations\n",
    "    forward = 0*h # forward difference\n",
    "    backward = 0*h # backward difference\n",
    "    center = 0*h # centered difference\n",
    "    compstep = 0*h # complex step\n",
    "    \n",
    "    # Loop through each h\n",
    "    count = 0\n",
    "    for i in h:\n",
    "        forward[count] = (f(x+i) - f(x))/i\n",
    "        backward[count] = (f(x) - f(x-i))/i\n",
    "        center[count] = 0.5*(forward[count]+ backward[count]) \n",
    "        compstep[count] = (f(x+i*1j)/i).imag\n",
    "        count += 1\n",
    "        \n",
    "    # Print results\n",
    "    if(loud):\n",
    "        print('h\\t forward\\tbackward\\tcentered\\tcomplex step') \n",
    "        for i in range(count):\n",
    "            print(\"%.5f\" % h[i],\"  %.11f\" % forward[i],\n",
    "                  \" %.11f\" % backward[i], \"  %.11f\" % center[i], \"  %.11f\" % compstep[i])\n",
    "\n",
    "    # Determine errors and plot\n",
    "    if(plot):\n",
    "        plt.loglog(h,np.fabs(forward-exact),'o-',label=\"Forward Difference\")\n",
    "        plt.loglog(h,np.fabs(backward-exact),'o-',label=\"Backward Difference\")\n",
    "        plt.loglog(h,np.fabs(center-exact),'o-',label=\"Central Difference\")\n",
    "        plt.loglog(h,np.fabs(compstep-exact),'o-',label=\"Complex Step\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.title(\"Absolute Error on Log-Log Scale\")\n",
    "        plt.xlabel(\"h\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.show()\n",
    "        \n",
    "    return forward,backward,center\n",
    "        \n",
    "# Define step sizes\n",
    "h = 2**np.linspace(-1,-10,10) #np.array([2**(-1),2**(-2),2**(-3),2**(-4),2**(-5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function\n",
    "\n",
    "$$f(x) = \\sqrt{x},~\\text{at}~x = 0.5.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define knowns\n",
    "f = lambda x: np.sqrt(x)\n",
    "x = 0.5\n",
    "exact = 0.70710678118\n",
    "\n",
    "# Run function\n",
    "forward,backward,center = DiffApproximations(f,h,x,exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function\n",
    "\n",
    "$$f(x) = \\arctan(x^2 - 0.9  x + 2)~\\text{at}~x=0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define knowns\n",
    "f = lambda x: np.arctan(x**2 - 0.9*x + 2)\n",
    "x = 0.5\n",
    "exact = 5/212\n",
    "\n",
    "# Run function\n",
    "forward,backward,center = DiffApproximations(f,h,x,exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function\n",
    "\n",
    "$$f(x) = J_0(x),~\\text{at}~x = 1,~\\text{where}~J_0(x)~\\text{is a Bessel function of the first kind given by}$$\n",
    "\n",
    "$$J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\, \\Gamma(m+\\alpha+1)} {\\left(\\frac{x}{2}\\right)}^{2m+\\alpha}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define knowns\n",
    "def J_0(x, M = 100):\n",
    "    \"\"\"Order zero Bessel function of the first-kind\n",
    "    evaluated at x\n",
    "    Inputs:\n",
    "        alpha:  value of alpha\n",
    "        x:      point to evaluate Bessel function at\n",
    "        M:      number of terms to include in sum\n",
    "    Returns:\n",
    "        J_0(x)\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for m in range(M):\n",
    "        total += (-1)**m/(math.factorial(m)*math.gamma(m+1))*(0.5*x)**(2*m)\n",
    "    return total \n",
    "x = 1\n",
    "exact = -0.4400505857449335\n",
    "\n",
    "# Run function\n",
    "forward,backward,center = DiffApproximations(J_0,h,x,exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function \n",
    "\n",
    "$$f(x) = e^{-\\frac{x^2}{\\sigma^2}}.$$\n",
    "\n",
    "We will use finite differences to estimate derivatives of this function when $\\sigma = 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using forward, backward, and centered differences evaluate the error in the function at 1000 points between $x=-1$ and $x=1$ ({\\tt np.linspace} will be useful) using the following values of $h$:\n",
    "\\[ h = 2^0, 2^{-1}, 2^{-2}, \\dots, 2^{-7}.\\]\n",
    "For each set of approximations compute the average absolute error over the one thousand points\n",
    "\\[ \\text{Average Absolute Error} = \\frac{1}{N} \\sum_{i=1}^{N} | f'(x_i) - f'_\\mathrm{approx}(x_i)|,\\]\n",
    "where $f'_\\mathrm{approx}(x_i)$ is the value of an approximate derivative at $x_i$ and $N$ is the number of points the function derivative is evaluated at. You will need to find the exact value of the derivative to complete this estimate.  \n",
    "\n",
    "Plot the value of the average absolute error error from each approximation on the same figure on a log-log scale. Discuss what you see.  Is the highest-order method always the most accurate?  Compute the order of accuracy you observe by computing the slope on the log-log plot.\n",
    "\n",
    "Next, compute the maximum absolute error for each value of $h$ as\n",
    "\\[\\text{Maximum Absolute Error} =  \\max_{i} | f'(x_i) - f'_\\mathrm{approx}(x_i)|.\\]\n",
    "\n",
    "Plot the value of the maximum absolute error error from each approximation on the same figure on a log-log scale. Discuss what you see.  Is the highest-order method always the most accurate?  \n",
    "\n",
    "- Repeat the previous part using the second-order version of the second-derivative approximation discussed above. You will only have one formula in this case. \n",
    "\\item Now derive a formula for the fourth derivative and predict its order of accuracy. Then repeat the calculation and graphing of the average and maximum absolute errors and verify the order of accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must know the exact first derivative, $f'(x)$, in order to determine the errors, therefore\n",
    "\n",
    "$$f'(x) = -\\frac{2x}{\\sigma^2}~e^{-\\frac{x^2}{\\sigma^2}} = -\\frac{2x}{\\sigma^2}~f(x).$$\n",
    "\n",
    "First, all of the constants, necessary functions and solution arrays are defined. The $\\texttt{NumPy}$ function $\\texttt{linspace}$ is used to define the evenly space values of $\\texttt{x}$. Then, empty arrays are created that will fill all of the needed errors for each method (errors for each point, average errors for each step, and maximum errors for each step).\n",
    "\n",
    "$\\br$A $\\texttt{for}$ loop is used to loop through the index of each $h$, and then another loop is used to loop through the index of each $x$. Each approximation is then solved using the equations given in the Chapter 13 lecture notes. Next, the individual errors, average errors, and maximum errors are all calculated per the equations given in the problem statement. Last, the slopes for each method are determined using the approximations between $h = 2^{-6}$ and $h = 2^{-7}$, which approximate the order of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define constants and functions\n",
    "N = 1000\n",
    "sigma = 0.1\n",
    "f = lambda x: np.exp(-x**2/sigma**2)\n",
    "fprime = lambda x: -2*x/sigma**2*f(x)\n",
    "\n",
    "# Define step sizes\n",
    "bases = 2*np.ones(8)\n",
    "powers = np.array([0,-1,-2,-3,-4,-5,-6,-7])\n",
    "h = np.power(bases,powers)\n",
    "\n",
    "# Define values of x\n",
    "x = np.linspace(-1,1,N)\n",
    "\n",
    "# Evaluate derivative at each x\n",
    "exact = fprime(x)\n",
    "\n",
    "# Define arrays to fill with approximations\n",
    "forward = np.zeros([h.size,x.size])\n",
    "backward = np.zeros([h.size,x.size])\n",
    "center = np.zeros([h.size,x.size])\n",
    "comp1 = np.zeros([h.size,x.size])\n",
    "comp2 = np.zeros([h.size,x.size])\n",
    "\n",
    "# Define errors for each h\n",
    "errorForward = np.zeros([h.size,x.size])\n",
    "errorBackward = np.zeros([h.size,x.size])\n",
    "errorCenter = np.zeros([h.size,x.size])\n",
    "errorComp1 = np.zeros([h.size,x.size])\n",
    "errorComp2 = np.zeros([h.size,x.size])\n",
    "avgErrorForward = np.zeros(h.size)\n",
    "avgErrorBackward = np.zeros(h.size)\n",
    "avgErrorCenter = np.zeros(h.size)\n",
    "avgErrorComp1 = np.zeros(h.size)\n",
    "avgErrorComp2 = np.zeros(h.size)\n",
    "maxErrorForward = np.zeros(h.size)\n",
    "maxErrorBackward = np.zeros(h.size)\n",
    "maxErrorCenter = np.zeros(h.size)\n",
    "maxErrorComp1 = np.zeros(h.size)\n",
    "maxErrorComp2 = np.zeros(h.size)\n",
    "\n",
    "# Loop through indicies of h for h_i\n",
    "for i in range(h.size):\n",
    "    # Loop through indicies x for x_j, solving for each x\n",
    "    for j in range(x.size):\n",
    "        forward[i,j] = (f(x[j]+h[i]) - f(x[j]))/h[i]\n",
    "        backward[i,j] = (f(x[j]) - f(x[j]-h[i]))/h[i]\n",
    "        center[i,j] = 0.5*(forward[i,j]+ backward[i,j]) \n",
    "        comp1[i,j] = (f(x[j] +h[i]*1j)/h[i]).imag\n",
    "        comp2[i,j] = 8/3/h[i]*(f(x[j] +h[i]*1j/2)-1/8*f(x[j]+h[i]*1j)).imag\n",
    "    # Determine individual errors for h_i\n",
    "    errorForward[i,:] = np.fabs(exact-forward[i,:])\n",
    "    errorBackward[i,:] = np.fabs(exact-backward[i,:])\n",
    "    errorCenter[i,:] = np.fabs(exact-center[i,:])\n",
    "    errorComp1[i,:] = np.fabs(exact-comp1[i,:])\n",
    "    errorComp2[i,:] = np.fabs(exact-comp2[i,:])\n",
    "    # Determine average absolute error for h_i\n",
    "    avgErrorForward[i] = np.sum(errorForward[i,:])/N\n",
    "    avgErrorBackward[i] = np.sum(errorBackward[i,:])/N\n",
    "    avgErrorCenter[i] = np.sum(errorCenter[i,:])/N\n",
    "    avgErrorComp1[i] = np.sum(errorComp1[i,:])/N\n",
    "    avgErrorComp2[i] = np.sum(errorComp2[i,:])/N\n",
    "    # Determine max absolute error for h_i\n",
    "    maxErrorForward[i] = errorForward[i,:].max()\n",
    "    maxErrorBackward[i] = errorBackward[i,:].max()\n",
    "    maxErrorCenter[i] = errorCenter[i,:].max()\n",
    "    maxErrorComp1[i] = errorComp1[i,:].max()\n",
    "    maxErrorComp2[i] = errorComp2[i,:].max()\n",
    "    \n",
    "# Determine slope between last two approximations\n",
    "slopeForward = (np.log(avgErrorForward[-1])-np.log(avgErrorForward[-2]))/(np.log(h[-1])-np.log(h[-2]))\n",
    "slopeBackward = (np.log(avgErrorBackward[-1])-np.log(avgErrorBackward[-2]))/(np.log(h[-1])-np.log(h[-2]))\n",
    "slopeCenter = (np.log(avgErrorCenter[-1])-np.log(avgErrorCenter[-2]))/(np.log(h[-1])-np.log(h[-2]))\n",
    "slopeComp1 = (np.log(avgErrorComp1[-1])-np.log(avgErrorComp1[-2]))/(np.log(h[-1])-np.log(h[-2]))\n",
    "slopeComp2 = (np.log(avgErrorComp2[-1])-np.log(avgErrorComp2[-2]))/(np.log(h[-1])-np.log(h[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average error for each method is then plotted for each method, on a log-log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average error\n",
    "plt.loglog(h,avgErrorForward,'o-',label=\"Forward difference\")\n",
    "plt.loglog(h,avgErrorBackward,'o-',label=\"Backward difference\")\n",
    "plt.loglog(h,avgErrorCenter,'o-',label=\"Central difference\")\n",
    "plt.loglog(h,avgErrorComp1,'o-',label=\"Comp. Step 1\")\n",
    "plt.loglog(h,avgErrorComp2,'o-',label=\"Comp. Step 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Average absolute error, log-log scale')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the methods are rather similar in the terms of error up until $h = 2^{-2}$, and then the error of the central difference method diverges from the others. Throughout the domain of $h$, the forward and backward methods have errors of the same magnitude. The central difference method has the least error throughout the entire domain (note that this may not be the case for other functions). Of interest is that the error increases for all three methods up until $h = 2^{-2}$. This is due to the fact that this is the region in which $h^2 \\approx h$, where the error then begins to decrease.\n",
    "\n",
    "$\\br$The estimates for the order of accuracy are then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print slopes for order accuracy\n",
    "print('Order accuracies')\n",
    "print('Forward difference\\t',\"%.5f\" % slopeForward)\n",
    "print('Backward difference\\t',\"%.5f\" % slopeBackward)\n",
    "print('Center difference\\t',\"%.5f\" % slopeCenter)\n",
    "print('Comp Step 1\\t',\"%.5f\" % slopeComp1)\n",
    "print('Comp Step 2\\t',\"%.5f\" % slopeComp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the forward and backward difference methods have the same order error. The divergence of the central difference method is also evident by the fact that it is of second-order error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum error\n",
    "plt.loglog(h,maxErrorForward,'o-',label=\"Forward difference\")\n",
    "plt.loglog(h,maxErrorBackward,'o-',label=\"Backward difference\")\n",
    "plt.loglog(h,maxErrorCenter,'o-',label=\"Central difference\")\n",
    "plt.loglog(h,maxErrorComp1,'o-',label=\"Comp Step 1\")\n",
    "plt.loglog(h,maxErrorComp2,'o-',label=\"Comp Step 1\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Maximum absolute error, log-log scale')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, the plot shows that the second-order method remains the most accurate in terms of maximum errors for all $h$. The increase in error for the first three step sizes of the forward and backward difference methods is more evident with the maximum error. Again, the orders of accuracy become more clear as $h \\rightarrow 0$. Of interest is that the maximum errors are generally an order of magnitude higher than the average errors, meaning that for some values of $x$ the approximation is significantly less accurate.\n",
    "\n",
    "$\\br$Next, we will estimate the second-derivative.\n",
    "\n",
    "$\\br$It is necessary to determine the exact second derivative, $f''(x)$, in order to determine the errors, therefore\n",
    "\n",
    "$$f''(x) = \\frac{4x^2 - 2\\sigma^2}{\\sigma^4}~e^{-\\frac{x^2}{\\sigma^2}} = \\frac{4x^2 - 2\\sigma^2}{\\sigma^4}~f(x).$$\n",
    "\n",
    "$\\br$The same constants are defined as were previously with the first-order approximation. In addition, the same set of $\\texttt{for}$ loops is used to solve for the approximations for each $x$ and $h$. The errors are then calculated, the order of accuracy approximated, and plots are made for the average absolute error and the maximum absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array to fill with approximations\n",
    "second = np.zeros([h.size,x.size])\n",
    "\n",
    "# Define errors for each h\n",
    "errorSecond = np.zeros([h.size,x.size])\n",
    "avgErrorSecond = np.zeros(h.size)\n",
    "maxErrorSecond = np.zeros(h.size)\n",
    "\n",
    "# Define exact solution and evaluate at x\n",
    "fprime2 = lambda x: (4*x**2-2*sigma**2)/sigma**4*f(x)\n",
    "exact2 = fprime2(x)\n",
    "\n",
    "# Loop through indicies of h for h_i\n",
    "for i in range(h.size):\n",
    "    # Loop through indicies x for x_j, solving for each x\n",
    "    for j in range(x.size):\n",
    "        second[i,j] = (f(x[j]+h[i])-2*f(x[j])+f(x[j]-h[i]))/h[i]**2\n",
    "    # Determine individual errors for h_i\n",
    "    errorSecond[i,:] = np.fabs(exact2-second[i,:])\n",
    "    # Determine average absolute error for h_i\n",
    "    avgErrorSecond[i] = np.sum(errorSecond[i,:])/N\n",
    "    # Determine max absolute error for h_i\n",
    "    maxErrorSecond[i] = errorSecond[i,:].max()       \n",
    "\n",
    "# Determine slope between last two approximations\n",
    "slopeSecond = (np.log(avgErrorSecond[-1])-np.log(avgErrorSecond[-2]))/(np.log(h[-1])-np.log(h[-2]))\n",
    "\n",
    "# Plot average error\n",
    "plt.loglog(h,avgErrorSecond,'o-')\n",
    "plt.title('Average absolute error, log-log scale')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Print slope for order accuracy\n",
    "print('Order accuracy')\n",
    "print('Second-derivative approximation\\t',\"%.5f\" % slopeSecond)\n",
    "\n",
    "# Plot maximum error\n",
    "plt.loglog(h,maxErrorSecond,'o-')\n",
    "plt.title('Maximum absolute error, log-log scale')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, we have second-order accuracy that is evident in both of the plots above. In addition, it is important to take note of the magnitude of the maximum errors compared to the magnitude of the average errors. In this case again, the maximum errors are significantly larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\br$Next... we will venture into creating our own formula to approximate the fourth derivative. As usual, we must first know the exact solution of the fourth derivative, which is \n",
    "\n",
    "$$f^{(4)}(x) = \\frac{4\\Big(3\\sigma^4 + 4x^4 - 12\\sigma^2x^2\\Big)}{\\sigma^8}e^{-\\frac{x^2}{\\sigma^2}} = \\frac{4\\Big(3\\sigma^4 + 4x^4 - 12\\sigma^2x^2\\Big)}{\\sigma^8}~f(x).$$\n",
    "\n",
    "Here, we will use a central difference method to determine the fourth derivative. There are many finite difference approximations that can be made for the fourth-derivative: forward, backward, centered, etc. As long as the process made results in a viable method, credit will be awarded.\n",
    "\n",
    "$\\br$First, we must start with the Taylor series expansion at $x+h$, $x-h$, $x+2h$, and $x-2h$:\n",
    "\n",
    "$$f(x+h) = f(x) + hf^{(1)}(x) + \\frac{h^2}{2}f^{(2)}(x) + \\frac{h^3}{6}f^{(3)}(x) + \\frac{h^4}{24}f^{(4)}(x) + \\frac{h^5}{120}f^{(5)}(x) + \\frac{h^6}{720}f^{(6)}(x) + O(h^7),$$\n",
    "\n",
    "$$f(x-h) = f(x) - hf^{(1)}(x) + \\frac{h^2}{2}f^{(2)}(x) - \\frac{h^3}{6}f^{(3)}(x) + \\frac{h^4}{24}f^{(4)}(x) - \\frac{h^5}{120}f^{(5)}(x) + \\frac{h^6}{720}f^{(6)}(x) + O(h^7),$$\n",
    "\n",
    "$$f(x+2h) = f(x) + 2hf^{(1)}(x) + 2h^2f^{(2)}(x) + \\frac{4h^3}{3}f^{(3)}(x) + \\frac{2h^4}{3}f^{(4)}(x) + \\frac{4h^5}{15}f^{(5)}(x) + \\frac{4h^6}{45}f^{(6)}(x) + O(h^7),$$\n",
    "\n",
    "and\n",
    "\n",
    "$$f(x-2h) = f(x) - 2hf^{(1)}(x) + 2h^2f^{(2)}(x) - \\frac{4h^3}{3}f^{(3)}(x) + \\frac{2h^4}{3}f^{(4)}(x) - \\frac{4h^5}{15}f^{(5)}(x) + \\frac{4h^6}{45}f^{(6)}(x) + O(h^7).$$\n",
    "\n",
    "Next, we will add the above four equations in a way such that the $h^2$ term is cancelled out. This will be done by adding $-2$ times the first two equations, and $1$ times the last two equations:\n",
    "\n",
    "$$-4f(x+h) - 4 f(x-h) + f(x+2h) + f(x-2h) = -6f(x) + h^4f^{(4)}(x) + \\frac{h^6}{6}f^{(6)}(x).$$\n",
    "\n",
    "The equation is then solved for the fourth derivative:\n",
    "\n",
    "$$f^{(4)}(x) = \\frac{f(x-2h) - 4f(x-h) + 6f(x) - 4f(x+h) + f(x+2h)}{h^4} - \\frac{h^2}{6}f^{(6)}(x)$$\n",
    "\n",
    "Taking care of the last term, we can consider that the remaining error is on the order of $h^2$.\n",
    "\n",
    "$$f^{(4)}(x) = \\frac{f(x-2h) - 4f(x-h) + 6f(x) - 4f(x+h) + f(x+2h)}{h^4} + O(h^2)$$\n",
    "\n",
    "Now, we have our centered finite difference approximation for the fourth derivative. Following the same process as done in the two parts above, we will evaluate its performance at varying values of $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array to fill with approximations\n",
    "fourth = np.zeros([h.size,x.size])\n",
    "\n",
    "# Define errors for each h\n",
    "errorFourth = np.zeros([h.size,x.size])\n",
    "avgErrorFourth = np.zeros(h.size)\n",
    "maxErrorFourth = np.zeros(h.size)\n",
    "\n",
    "# Define exact solution and evaluate at x\n",
    "fprime4 = lambda x: 4*f(x)*(3*sigma**4+4*x**4-12*sigma**2*x**2)/sigma**8\n",
    "exact4 = fprime4(x)\n",
    "\n",
    "# Loop through indicies of h for h_i\n",
    "for i in range(h.size):\n",
    "    # Loop through indicies x for x_j, solving for each x\n",
    "    for j in range(x.size):\n",
    "        fourth[i,j] = (f(x[j]-2*h[i])-4*f(x[j]-h[i])+6*f(x[j])-4*f(x[j]+h[i])+f(x[j]+2*h[i]))/h[i]**4\n",
    "    # Determine individual errors for h_i\n",
    "    errorFourth[i,:] = np.fabs(exact4-fourth[i,:])\n",
    "    # Determine average absolute error for h_i\n",
    "    avgErrorFourth[i] = np.sum(errorFourth[i,:])/N\n",
    "    # Determine max absolute error for h_i\n",
    "    maxErrorFourth[i] = errorSecond[i,:].max()       \n",
    "\n",
    "# Determine slope between last two approximations\n",
    "slopeFourth = (np.log(avgErrorFourth[-1])-np.log(avgErrorFourth[-2]))/(np.log(h[-1])-np.log(h[-2]))\n",
    "\n",
    "# Plot average error\n",
    "plt.loglog(h,avgErrorFourth,'o-')\n",
    "plt.title('Average absolute error, log-log scale')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Print slope for order accuracy\n",
    "print('Order accuracy')\n",
    "print('Fourth-derivative approximation\\t',\"%.5f\" % slopeFourth)\n",
    "\n",
    "# Plot maximum error\n",
    "plt.loglog(h,maxErrorFourth,'o-')\n",
    "plt.title('Maximum absolute error, log-log scale')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation of the slope at the last two points leads to an order of accuracy of 2, as we expected in the formulation of our method above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
